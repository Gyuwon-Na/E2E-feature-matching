import torch
import torch.nn.functional as F
import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import kornia
from kornia.feature import LoFTR
from train import CliffordNetwork, Phase1Preprocessor, Phase4Refinement, HomographyAugmentor

# ==========================================
# ğŸ”µ LoFTR ë§¤ì¹­ ê²°ê³¼ ì¶”ì¶œ í•¨ìˆ˜
# ==========================================
def get_loftr_matches(img_src, img_tgt, device):
    """
    Korniaë¥¼ ì‚¬ìš©í•˜ì—¬ LoFTR ë§¤ì¹­ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    matcher = LoFTR(pretrained='outdoor').to(device).eval()
    
    # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜ ë° í…ì„œí™”
    img0 = cv2.cvtColor(img_src, cv2.COLOR_RGB2GRAY)
    img1 = cv2.cvtColor(img_tgt, cv2.COLOR_RGB2GRAY)
    
    timg0 = torch.from_numpy(img0)[None, None].float().to(device) / 255.
    timg1 = torch.from_numpy(img1)[None, None].float().to(device) / 255.
    
    with torch.no_grad():
        correspondences = matcher({'image0': timg0, 'image1': timg1})
        
    mkpts0 = correspondences['keypoints0'].cpu().numpy()
    mkpts1 = correspondences['keypoints1'].cpu().numpy()
    
    return mkpts0, mkpts1

# ==========================================
# ğŸ“ í†µí•© ë¹„êµ ì‹œê°í™” (LoFTR vs Ours)
# ==========================================
def visualize_with_error_heatmap(
    img_src, img_tgt, ours_data, H_mat, threshold=5.0, conf_thresh=0.5
):
    """
    1. ì™¼ìª½: ë§¤ì¹­ ê²°ê³¼ (Green/Red) - ì´ë¯¸ì§€ B ë°–ìœ¼ë¡œ ë‚˜ê°€ëŠ” ì ì€ ì œì™¸
    2. ì˜¤ë¥¸ìª½: ì—ëŸ¬ íˆíŠ¸ë§µ - ì–´ë””ì„œ ì˜¤ì°¨ê°€ í°ì§€ ì‹œê°í™”
    """
    H, W, _ = img_src.shape
    init_verts, final_verts, confidences = ours_data
    
    # 1. ì¢Œí‘œ ê³„ì‚° ë° ë³€í™˜
    src_pts_norm = init_verts.detach().cpu().numpy()
    src_x = (src_pts_norm[:, 0] + 1) * 0.5 * W
    src_y = (src_pts_norm[:, 1] + 1) * 0.5 * H
    
    pred_pts_norm = final_verts.detach().cpu().numpy()
    pred_x = (pred_pts_norm[:, 0] + 1) * 0.5 * W
    pred_y = (pred_pts_norm[:, 1] + 1) * 0.5 * H
    
    # ì •ë‹µ ìœ„ì¹˜ ê³„ì‚°
    src_pts_pixel = np.stack([src_x, src_y], axis=1).reshape(-1, 1, 2)
    gt_pts = cv2.perspectiveTransform(src_pts_pixel, H_mat).reshape(-1, 2)
    
    # 2. ë§ˆìŠ¤í¬ ìƒì„± (ìœ íš¨ ì˜ì—­ + ì‹ ë¢°ë„)
    mask_range = (gt_pts[:, 0] >= 0) & (gt_pts[:, 0] < W) & \
                 (gt_pts[:, 1] >= 0) & (gt_pts[:, 1] < H)
    mask_conf = confidences >= conf_thresh
    final_mask = mask_range & mask_conf
    
    # 3. ì—ëŸ¬ ê³„ì‚° (í”½ì…€ ê±°ë¦¬ ì˜¤ì°¨)
    # ëª¨ë“  ì ì— ëŒ€í•´ ê³„ì‚°í•˜ë˜, ë§ˆìŠ¤í¬ë˜ì§€ ì•Šì€ ê³³ì€ ë‚˜ì¤‘ì— í•„í„°ë§
    errors = np.linalg.norm(np.stack([pred_x, pred_y], axis=1) - gt_pts, axis=1)
    
    plt.figure(figsize=(20, 10))
    
    # --- [ì™¼ìª½] ë§¤ì¹­ ê²°ê³¼ ì‹œê°í™” ---
    plt.subplot(1, 2, 1)
    canvas = np.hstack((img_src, img_tgt))
    plt.imshow(canvas)
    
    valid_indices = np.where(final_mask)[0]
    if len(valid_indices) > 0:
        # ê°€ë…ì„±ì„ ìœ„í•´ ì¼ë¶€ ì ë§Œ ìƒ˜í”Œë§í•˜ì—¬ ê·¸ë¦¬ê¸°
        show_idx = np.random.choice(valid_indices, min(150, len(valid_indices)), replace=False)
        for i in show_idx:
            color = 'lime' if errors[i] < threshold else 'red'
            # ì˜¤íƒ€ ìˆ˜ì •: pred_py -> pred_y
            plt.plot([src_x[i], pred_x[i] + W], [src_y[i], pred_y[i]], color=color, lw=0.8, alpha=0.6)
            plt.scatter(src_x[i], src_y[i], c=color, s=5)
            plt.scatter(pred_x[i] + W, pred_y[i], c=color, s=5)
            
        acc = (errors[valid_indices] < threshold).mean() * 100
        plt.text(10, 30, f"Acc: {acc:.1f}% ({len(valid_indices)} pts)", 
                 color='white', backgroundcolor='black', fontsize=12)

    plt.title(f"Matching Results (Green < {threshold}px)", fontsize=15)
    plt.axis('off')

    # --- [ì˜¤ë¥¸ìª½] ì—ëŸ¬ íˆíŠ¸ë§µ (Error Heatmap) ---
    plt.subplot(1, 2, 2)
    
    # ê²©ì í•´ìƒë„ ë³µì› (ì˜ˆ: 32x32)
    grid_res = int(np.sqrt(len(errors)))
    # ì—ëŸ¬ ë§µ ì´ˆê¸°í™” (ìœ íš¨í•˜ì§€ ì•Šì€ ê³³ì€ ì—ëŸ¬ 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ë°°ê²½ìƒ‰ ì²˜ë¦¬)
    error_map_flat = np.zeros_like(errors)
    error_map_flat[final_mask] = errors[final_mask]
    
    error_map = error_map_flat.reshape(grid_res, grid_res)
    
    # íˆíŠ¸ë§µì„ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ í™•ëŒ€
    # INTER_CUBICì„ ì¨ì•¼ ì—ëŸ¬ ë¶„í¬ê°€ ë¶€ë“œëŸ½ê²Œ ë³´ì…ë‹ˆë‹¤.
    error_heatmap = cv2.resize(error_map, (W, H), interpolation=cv2.INTER_CUBIC)
    
    plt.imshow(img_src)
    # 'jet' ë§µ: íŒŒë€ìƒ‰(ì—ëŸ¬ ë‚®ìŒ) -> ë¹¨ê°„ìƒ‰(ì—ëŸ¬ ë†’ìŒ)
    im = plt.imshow(error_heatmap, cmap='jet', alpha=0.5)
    plt.colorbar(im, label='Pixel Error Distance')
    plt.title("Error Heatmap (Red = High Error Area)", fontsize=15)
    plt.axis('off')

    plt.tight_layout()
    plt.show()

# ==========================================
# ğŸš€ ë©”ì¸ í…ŒìŠ¤íŠ¸ ë¡œì§
# ==========================================
def run_evaluation(img_path, model_path, sam_path):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”¹ Running evaluation on {device}...")

    # 1. ëª¨ë¸ ë¡œë“œ (ë‚´ ëª¨ë¸)
    model = CliffordNetwork().to(device)
    if not os.path.exists(model_path):
        print(f"âŒ Model file not found: {model_path}")
        return
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval() 
    
    preprocessor = Phase1Preprocessor(sam_checkpoint=sam_path, model_type="vit_b", device=device)
    augmentor = HomographyAugmentor(128, 128)

    # 2. ì´ë¯¸ì§€ ì¤€ë¹„
    img_orig = cv2.imread(img_path)
    if img_orig is None:
        print("âŒ Image load failed.")
        return
    img_orig = cv2.resize(img_orig, (128, 128))
    img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)

    # ì „ì²˜ë¦¬ ë° Target ìƒì„±
    data = preprocessor.process_from_array(img_rgb)
    hsi_A, sdf_A, hsi_B, sdf_B, _, H_t = augmentor(None, data['hsi'], data['sdf'])
    
    hsi_A = hsi_A.unsqueeze(0).to(device)
    sdf_A = sdf_A.unsqueeze(0).to(device)
    hsi_B = hsi_B.unsqueeze(0).to(device)
    sdf_B = sdf_B.unsqueeze(0).to(device)

    H_mat = H_t.numpy()
    img_tgt_rgb = cv2.warpPerspective(img_rgb, H_mat, (128, 128))

    # 3. ë‚´ ëª¨ë¸ ì¶”ë¡ 
    print("ğŸ§  Extracting Ours Features...")
    with torch.no_grad():
        feat_A, feat_B = model(hsi_A, sdf_A, hsi_B, sdf_B)
        # ğŸ”¥ graph ë¶„ë¦¬
        feat_A = feat_A.detach().clone()
        feat_B = feat_B.detach().clone()

    # 4. Phase 4 ì •ì œ (ë‚´ ëª¨ë¸)
    print("ğŸ”§ Phase 4 Refinement...")
    refiner = Phase4Refinement(128, 128).to(device)  # ğŸ”¥ device ì „ë‹¬
    refiner.solver.lambdas['reg'] = 0.0001
    refiner.solver.lambdas['data'] = 100.0
    
    # ğŸ”¥ source_feat_initë„ detach + clone
    with torch.no_grad():
        init_mesh_verts = refiner.mesh.vertices.detach().clone()
        source_feat_init = refiner.solver.sample_features(feat_A, init_mesh_verts)
        source_feat_init = source_feat_init.detach().clone()
    
    # 300 steps ìµœì í™”
    final_verts, _ = refiner(feat_B, source_feat_init, steps=300)

    # ğŸ”¥ 5. ì‹ ë¢°ë„(Confidence) ê³„ì‚° (ì¥ì¹˜ í• ë‹¹ ìˆ˜ì •)
    print("ğŸ“Š Computing Confidence Scores...")
    with torch.no_grad():
        # final_vertsëŠ” ì´ë¯¸ cudaì— ìˆì§€ë§Œ, 
        # refiner.mesh.initial_verticesëŠ” cpuì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ .to(device) ì¶”ê°€
        src_init_verts = refiner.mesh.initial_vertices.to(device) 
        
        pred_feat = refiner.solver.sample_features(feat_B, final_verts)
        init_feat = refiner.solver.sample_features(feat_A, src_init_verts)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        confidences = F.cosine_similarity(pred_feat, init_feat, dim=1).cpu().numpy()
    
    print(f"   Confidence Stats: min={confidences.min():.3f}, max={confidences.max():.3f}, mean={confidences.mean():.3f}")

    # 6. LoFTR ë§¤ì¹­ ìˆ˜í–‰
    print("ğŸ¥Š Running LoFTR Baseline...")
    loftr_kpts = get_loftr_matches(img_rgb, img_tgt_rgb, device)

    # 7. ê²°ê³¼ ë¹„êµ ì‹œê°í™”
    print("ğŸ¨ Visualizing Comparison...")
    visualize_with_error_heatmap(
        img_rgb, img_tgt_rgb, 
        ours_data=(refiner.mesh.initial_vertices, final_verts, confidences),  # ğŸ”¥ confidences ì¶”ê°€
        H_mat=H_mat,
        threshold=15.0,
        conf_thresh=0.5  # ğŸ”¥ ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •
    )

if __name__ == "__main__":
    TEST_IMAGE = "./img/val2017/000000579893.jpg" 
    MODEL_PATH = "clifford_model_final.pth"
    SAM_PATH = "sam_vit_b_01ec64.pth"
    
    run_evaluation(TEST_IMAGE, MODEL_PATH, SAM_PATH)